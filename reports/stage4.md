### Результаты профилирования
**Обстрел put-ами**
Параметры: -t64 -c64 -R8000

```
 Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.60ms  656.08us  13.54ms   65.35%
    Req/Sec   132.07     50.15   300.00     70.63%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.58ms
 75.000%    2.07ms
 90.000%    2.50ms
 99.000%    3.08ms
 99.900%    3.42ms
 99.990%    5.21ms
 99.999%    8.73ms
100.000%   13.54ms
----------------------------------------------------------
 960037 requests in 2.00m, 69.99MB read
Requests/sec:   8000.40
Transfer/sec:    597.22KB
```
8000 запросов в секунду вывозит, но из 9000 осиливает только ~8400.

Для наглядности добавлю Latency с 3 этапа:
```
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.53ms
 75.000%    2.02ms
 90.000%    2.44ms
 99.000%    3.01ms
 99.900%    3.36ms
 99.990%    6.18ms
 99.999%    8.53ms
100.000%    9.56ms
```
99.999% запросов получали ответ за время, сопостовимое с прошлым этапом, и только на отметке 100.000% появляется рост времени в 1,5 раза. Предположим, что там оказались пара несчастных запросов, на которые выпал flush - отсюда разница со средним, и плюс проверка хэша и проксирование, которые могли дать замедление относительно 3 стадии.


ALLOC
11% - выяснение ноды по ключу, рандеву хэширование (на следующем этапе станет больше, но пока живём)
13% - формирование респонса
15% - вставка в DAO
46% - доля сетевого взаимодействия не особо изменилась при добавлении проксирования. Видимо здесь промежуток времени, когда proxy() вызывался не очень часто (см. ALLOC при обстреле get-ами) и тем самым не откусил свой процент.
[ALLOC PUT](reports/profiling_results/allocput4.svg)

CPU
1% - вставка в DAO (каждый раз забавно, что при обстреле путами, пут отжирает минимум cpu)
6% - формирование респонса
11% - чтение из сокета
22% - обеспечение работы пула экзекуторов (опять не прикольно, что работа экзекутора - 7%, а обёртка их в BlockingQueue - в 3 раза больше)
26% - отправка респонса
За несколько прогонов не было построено графа, который значимо бы отличался по CPU от прошлого этапа -> разбивка по трём нодам погоды не сделала.
[CPU PUT](reports/profiling_results/cpuput4.svg)

LOCK
54% - под локом на каком-то из экзекуторов сервис пытается отослать респонс
46% - ArrayBlockingQueue, в которой лежат экзекуторы
[LOCK PUT](reports/profiling_results/lockput4.svg)

Обстрел get-ами
Параметры: -t16 -c16 -R8000

```
 Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.60ms  657.38us   9.22ms   65.00%
    Req/Sec   132.05     50.22   222.00     70.58%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.58ms
 75.000%    2.08ms
 90.000%    2.50ms
 99.000%    3.07ms
 99.900%    3.39ms
 99.990%    4.57ms
 99.999%    7.59ms
100.000%    9.23ms
----------------------------------------------------------
  960031 requests in 2.00m, 74.19MB read
Requests/sec:   8000.53
Transfer/sec:    633.11KB
```
8000 requests/sec из 8000 обещанных. _SUCCESS

Latency 3 стадии:
```
 Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.16ms
 75.000%    1.59ms
 90.000%    2.03ms
 99.000%    2.72ms
 99.900%    3.04ms
 99.990%    3.43ms
 99.999%    5.84ms
100.000%    7.86ms
```
С шардированием get замедлился процентов на 20-30. Тут разница побольше, но ведь терпимо же?)

ALLOC
3% - выяснение ноды по ключу
5% - формирование реквеста
11% - поиск в DAO
31% - чтения из селекторов
49% - проксирование... Казалось бы, это просто метод, дублирующий запрос на указанный порт, но видимо invoke() для установки соединения нужно нормально так отхватить памяти. Вообще неожиданно, что будет такая большая доля. (49%, КАРЛ)
[ALLOC GET](reports/profiling_results/allocget4.svg)

CPU
Искала следы проксирования на CPU, но картинка аналогичная прошлому этапу, зацепиться вроде бы не за что.
[CPU GET](reports/profiling_results/cpuget4.svg)

LOCK
Тут тоже ничего нового. Под локами работают пул экзекуторов, сам get и отправка респонса.
[LOCK GET](reports/profiling_results/lockget4.svg)